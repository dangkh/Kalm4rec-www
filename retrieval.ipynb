{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c3d9da-681e-47c3-8c8e-4baad190e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import world\n",
    "import os\n",
    "import time\n",
    "from retrievalHelper.utils import *\n",
    "from retrievalHelper.u4Res import *\n",
    "from retrievalHelper.u4KNN import *\n",
    "from retrievalHelper.u4train import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_, constant_, xavier_uniform_\n",
    "import torch.optim as optim\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fffa78e6-3b20-4ae9-a9b2-129314c2beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['city'] = \"Edinburgh\"\n",
    "config['checkKeyword'] = False\n",
    "config['showCheck']= False\n",
    "config['quantity'] = 20\n",
    "config['seed'] = 1001\n",
    "config['edgeType']  = \"IUF\"\n",
    "config['logResult'] = \"log\"\n",
    "config['export2LLMs'] = True\n",
    "config['RetModel'] = \"CBR\"\n",
    "config['numKW4FT'] = 20\n",
    "\n",
    "modelConfig = {}\n",
    "modelConfig['hidden_dim'] = 32\n",
    "modelConfig['learning_rate'] = 1e-3\n",
    "modelConfig['num_epochs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f314a6fe-0684-4fd6-93c1-c4a16a99f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running retrieval... \n",
      "Starting time:  2024-05-16 02:49:18\n",
      "INFO:  {'city': 'Edinburgh', 'checkKeyword': False, 'showCheck': False, 'quantity': 20, 'seed': 1001, 'edgeType': 'IUF', 'logResult': 'log', 'export2LLMs': True, 'RetModel': 'CBR', 'numKW4FT': 20}\n",
      "Loading training keyword\n",
      "info from extracted file\n",
      "Number of keyword: 75152\n",
      "info from extracted file\n",
      "Number of keyword: 20495\n",
      "number of review:  12753\n"
     ]
    }
   ],
   "source": [
    "print(\"Running retrieval... \")\n",
    "print(\"Starting time: \", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "print(\"INFO: \", config)\n",
    "\n",
    "city = config['city']\n",
    "print(\"Loading training keyword\")\n",
    "trainDat, testDat = data_reviewLoader(city)\n",
    "\n",
    "train_users, train_users2kw = extract_users(trainDat['np2users'])\n",
    "test_users, test_users2kw = extract_users(testDat['np2users'])\n",
    "\n",
    "# extract user2rest for label\n",
    "groundtruth = load_groundTruth(f'./data/reviews/{city}.csv')\n",
    "\n",
    "keywordScore, keywordFrequence = load_kwScore(city, config[\"edgeType\"])\n",
    "\n",
    "\n",
    "restGraph = retaurantReviewG([trainDat, keywordScore, keywordFrequence, \\\n",
    "                                config[\"quantity\"], config[\"edgeType\"], groundtruth])\n",
    "\n",
    "KNN = neighbor4kw(f'{city}_kwSenEB_pad', testDat,  restGraph)\n",
    "rest_Label = getRestLB(trainDat['np2rests'])\n",
    "\n",
    "sourceFile = open(config[\"logResult\"], 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48a725b-ae48-4dad-9cdf-8e2a53aa5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp, mr, mf = 0, 0, 0\n",
    "dictionary = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f0886c-034d-469c-a348-8647bbee29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1187/1187 [00:00<00:00, 1379.05it/s]\n",
      "/opt/anaconda3/envs/graph/lib/python3.9/site-packages/threadpoolctl.py:1223: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████| 1187/1187 [00:00<00:00, 14358.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 223/223 [00:00<00:00, 12756.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape (train / test):\n",
      "(1187, 20, 384) (223, 20, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "userFT, userFT_test, rest_feature = KNN.loadFT(config[\"numKW4FT\"], rest_Label, config[\"city\"])\n",
    "label_train, label_test = label_ftColab(train_users, test_users, groundtruth, restGraph.numRest, rest_Label)\n",
    "dim_users, dim_items = 384, 384\n",
    "learning_rate = modelConfig[\"learning_rate\"]\n",
    "hidden_dim = modelConfig['hidden_dim']\n",
    "num_epochs = modelConfig['num_epochs']\n",
    "\n",
    "trainLB = np.asarray(label_train)\n",
    "testLB = np.asarray(label_test)\n",
    "print(\"feature shape (train / test):\")\n",
    "print(userFT.shape, userFT_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c037d08-e5a7-4dc1-bb14-086678e6f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataCF(userFT, trainLB)\n",
    "test_dataset = DataCF(userFT_test, testLB)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "rest_feature = torch.from_numpy(rest_feature).type(torch.FloatTensor)\n",
    "rest_feature = rest_feature.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f4cd2f-1b38-4a65-8bc2-52c1bdb81593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_, constant_, xavier_uniform_\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, dim_users, dim_items, embedding_dim):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embeddings = nn.Linear(dim_users, embedding_dim)\n",
    "        self.uAP = AttentionPooling(embedding_dim , embedding_dim // 4)\n",
    "        self.item_embeddings = nn.Linear(dim_items, embedding_dim)\n",
    "        self.iAP = AttentionPooling(embedding_dim , embedding_dim // 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.activate = nn.Sigmoid()\n",
    "        # initial weight\n",
    "        self.apply(xavier_normal_initialization)\n",
    "\n",
    "    def forward(self, user_ft, item_ft):\n",
    "        numBatch = len(user_ft)\n",
    "        numRest = len(item_ft)\n",
    "        user_embeddings = self.user_embeddings(user_ft)\n",
    "        # user_embeddings = self.r1(user_embeddings)\n",
    "        user_embeddings = self.dropout(user_embeddings)\n",
    "        user_embeddings, _ = self.uAP(user_embeddings)\n",
    "        item_embeddings = self.item_embeddings(item_ft)\n",
    "        item_embeddings = self.dropout(item_embeddings)\n",
    "        # item_embeddings = self.r2(item_embeddings)\n",
    "        item_embeddings, _ = self.iAP(item_embeddings)\n",
    "        item_embeddings = torch.permute(item_embeddings, (1, 0))\n",
    "        pred = self.activate(torch.matmul(user_embeddings, item_embeddings))\n",
    "        # using this to return \n",
    "        return pred   \n",
    "\n",
    "\n",
    "    def prediction(self, user_ft, item_ft):\n",
    "        return self.forward(user_ft, item_ft)\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "\n",
    "        # Linear layers for attention scoring\n",
    "        self.V = nn.Linear(input_size, hidden_size)\n",
    "        self.size = input_size\n",
    "        self.w = nn.Linear(hidden_size, 1)\n",
    "        self.tanh = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # Calculate attention scores\n",
    "        scores = self.tanh(self.V(input_features)) \n",
    "        scores = self.w(scores)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = self.softmax(scores)\n",
    "\n",
    "        # Apply attention weights to input features\n",
    "        pooled_features = torch.sum(weights * input_features, dim=1)\n",
    "\n",
    "        return pooled_features, weights\n",
    "\n",
    "def xavier_normal_initialization(module):\n",
    "    r\"\"\" using `xavier_normal_`_ in PyTorch to initialize the parameters in\n",
    "    nn.Embedding and nn.Linear layers. For bias in nn.Linear layers,\n",
    "    using constant 0 to initialize.\n",
    "    .. _`xavier_normal_`:\n",
    "        https://pytorch.org/docs/stable/nn.init.html?highlight=xavier_normal_#torch.nn.init.xavier_normal_\n",
    "    Examples:\n",
    "        >>> self.apply(xavier_normal_initialization)\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        xavier_normal_(module.weight.data)\n",
    "        if module.bias is not None:\n",
    "            constant_(module.bias.data, 0)  \n",
    "def extractResult(lResults):\n",
    "    p = [x[0] for x in lResults]\n",
    "    r = [x[1] for x in lResults]\n",
    "    f = [x[2] for x in lResults]\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f48913-ea42-44df-afad-79ac903a6e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixFactorization(\n",
      "  (user_embeddings): Linear(in_features=384, out_features=32, bias=True)\n",
      "  (uAP): AttentionPooling(\n",
      "    (V): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (w): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (tanh): Sigmoid()\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (item_embeddings): Linear(in_features=384, out_features=32, bias=True)\n",
      "  (iAP): AttentionPooling(\n",
      "    (V): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (w): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (tanh): Sigmoid()\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (activate): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(dim_users, dim_items, hidden_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6677a9-8816-4ad7-9f79-e1f8e3577347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.05546272173523903\n",
      "Epoch [2/1], Loss: 0.20132334530353546\n",
      "Epoch [3/1], Loss: 0.043773941695690155\n",
      "Epoch [4/1], Loss: 1.9901691675186157\n",
      "Epoch [5/1], Loss: 0.04966359585523605\n",
      "Epoch [6/1], Loss: 0.11673018336296082\n",
      "Epoch [7/1], Loss: 0.18982993066310883\n",
      "Epoch [8/1], Loss: 0.1188897043466568\n",
      "Epoch [9/1], Loss: 0.19427472352981567\n",
      "Epoch [10/1], Loss: 0.14181426167488098\n",
      "Epoch [10/1], prec: 0.0008968609865470852, rec: 0.0009777889267843522, f1: 0.0008223368786411005\n",
      "Epoch [11/1], Loss: 0.370719313621521\n",
      "Epoch [12/1], Loss: 0.319380521774292\n",
      "Epoch [13/1], Loss: 0.1858205646276474\n",
      "Epoch [14/1], Loss: 0.27345970273017883\n",
      "Epoch [15/1], Loss: 0.25725066661834717\n",
      "Epoch [16/1], Loss: 0.49446627497673035\n",
      "Epoch [17/1], Loss: 0.46936720609664917\n",
      "Epoch [18/1], Loss: 0.3756892681121826\n",
      "Epoch [19/1], Loss: 0.570432186126709\n",
      "Epoch [20/1], Loss: 0.4567241072654724\n",
      "Epoch [20/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [21/1], Loss: 0.43314072489738464\n",
      "Epoch [22/1], Loss: 0.42747077345848083\n",
      "Epoch [23/1], Loss: 0.2889598309993744\n",
      "Epoch [24/1], Loss: 0.2778259217739105\n",
      "Epoch [25/1], Loss: 0.3264617919921875\n",
      "Epoch [26/1], Loss: 0.5804523229598999\n",
      "Epoch [27/1], Loss: 0.4680130183696747\n",
      "Epoch [28/1], Loss: 0.26657694578170776\n",
      "Epoch [29/1], Loss: 0.37381860613822937\n",
      "Epoch [30/1], Loss: 0.570238471031189\n",
      "Epoch [30/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [31/1], Loss: 0.3746764361858368\n",
      "Epoch [32/1], Loss: 0.40698856115341187\n",
      "Epoch [33/1], Loss: 0.5579632520675659\n",
      "Epoch [34/1], Loss: 0.3635566830635071\n",
      "Epoch [35/1], Loss: 0.4546481966972351\n",
      "Epoch [36/1], Loss: 0.6899661421775818\n",
      "Epoch [37/1], Loss: 0.388732373714447\n",
      "Epoch [38/1], Loss: 0.30956771969795227\n",
      "Epoch [39/1], Loss: 0.6374546885490417\n",
      "Epoch [40/1], Loss: 0.3304058611392975\n",
      "Epoch [40/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [41/1], Loss: 0.40943849086761475\n",
      "Epoch [42/1], Loss: 0.5687617659568787\n",
      "Epoch [43/1], Loss: 0.545630156993866\n",
      "Epoch [44/1], Loss: 0.3681175410747528\n",
      "Epoch [45/1], Loss: 0.5968331098556519\n",
      "Epoch [46/1], Loss: 0.3406602740287781\n",
      "Epoch [47/1], Loss: 0.32388606667518616\n",
      "Epoch [48/1], Loss: 0.45620349049568176\n",
      "Epoch [49/1], Loss: 0.6473815441131592\n",
      "Epoch [50/1], Loss: 0.29757291078567505\n",
      "Epoch [50/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [51/1], Loss: 0.38114652037620544\n",
      "Epoch [52/1], Loss: 0.5138015151023865\n",
      "Epoch [53/1], Loss: 0.3727424442768097\n",
      "Epoch [54/1], Loss: 0.23250740766525269\n",
      "Epoch [55/1], Loss: 0.43043193221092224\n",
      "Epoch [56/1], Loss: 0.5108861327171326\n",
      "Epoch [57/1], Loss: 0.5890357494354248\n",
      "Epoch [58/1], Loss: 0.39157894253730774\n",
      "Epoch [59/1], Loss: 0.44359466433525085\n",
      "Epoch [60/1], Loss: 0.41034960746765137\n",
      "Epoch [60/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [61/1], Loss: 0.39771056175231934\n",
      "Epoch [62/1], Loss: 0.570909857749939\n",
      "Epoch [63/1], Loss: 0.5340511202812195\n",
      "Epoch [64/1], Loss: 0.5077205896377563\n",
      "Epoch [65/1], Loss: 0.3702026307582855\n",
      "Epoch [66/1], Loss: 0.6716831922531128\n",
      "Epoch [67/1], Loss: 0.38670945167541504\n",
      "Epoch [68/1], Loss: 0.7034849524497986\n",
      "Epoch [69/1], Loss: 0.40762919187545776\n",
      "Epoch [70/1], Loss: 0.4956738352775574\n",
      "Epoch [70/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [71/1], Loss: 0.7193030118942261\n",
      "Epoch [72/1], Loss: 0.3683508634567261\n",
      "Epoch [73/1], Loss: 0.4589037001132965\n",
      "Epoch [74/1], Loss: 0.37275514006614685\n",
      "Epoch [75/1], Loss: 0.5020129084587097\n",
      "Epoch [76/1], Loss: 0.5656070113182068\n",
      "Epoch [77/1], Loss: 0.49770212173461914\n",
      "Epoch [78/1], Loss: 0.3269505500793457\n",
      "Epoch [79/1], Loss: 0.3159956932067871\n",
      "Epoch [80/1], Loss: 0.2621519863605499\n",
      "Epoch [80/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [81/1], Loss: 0.30209678411483765\n",
      "Epoch [82/1], Loss: 0.45120489597320557\n",
      "Epoch [83/1], Loss: 0.5059465765953064\n",
      "Epoch [84/1], Loss: 0.5810041427612305\n",
      "Epoch [85/1], Loss: 0.5003359913825989\n",
      "Epoch [86/1], Loss: 0.36062660813331604\n",
      "Epoch [87/1], Loss: 0.6056819558143616\n",
      "Epoch [88/1], Loss: 0.5061352849006653\n",
      "Epoch [89/1], Loss: 0.40686360001564026\n",
      "Epoch [90/1], Loss: 0.4214869737625122\n",
      "Epoch [90/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [91/1], Loss: 0.5880063772201538\n",
      "Epoch [92/1], Loss: 0.3498850166797638\n",
      "Epoch [93/1], Loss: 0.29729554057121277\n",
      "Epoch [94/1], Loss: 0.25275614857673645\n",
      "Epoch [95/1], Loss: 0.4033565819263458\n",
      "Epoch [96/1], Loss: 0.31946489214897156\n",
      "Epoch [97/1], Loss: 0.7381317615509033\n",
      "Epoch [98/1], Loss: 0.5709429979324341\n",
      "Epoch [99/1], Loss: 0.4259454011917114\n",
      "Epoch [100/1], Loss: 0.31677958369255066\n",
      "Epoch [100/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [101/1], Loss: 0.47363245487213135\n",
      "Epoch [102/1], Loss: 0.2859404683113098\n",
      "Epoch [103/1], Loss: 0.44285768270492554\n",
      "Epoch [104/1], Loss: 0.7151238918304443\n",
      "Epoch [105/1], Loss: 0.5709009766578674\n",
      "Epoch [106/1], Loss: 0.3593052625656128\n",
      "Epoch [107/1], Loss: 0.49837109446525574\n",
      "Epoch [108/1], Loss: 0.32170599699020386\n",
      "Epoch [109/1], Loss: 0.374850869178772\n",
      "Epoch [110/1], Loss: 0.5246835350990295\n",
      "Epoch [110/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [111/1], Loss: 0.3930763006210327\n",
      "Epoch [112/1], Loss: 0.3472748398780823\n",
      "Epoch [113/1], Loss: 0.5465874671936035\n",
      "Epoch [114/1], Loss: 0.3160124123096466\n",
      "Epoch [115/1], Loss: 0.36810749769210815\n",
      "Epoch [116/1], Loss: 0.32784605026245117\n",
      "Epoch [117/1], Loss: 0.31320035457611084\n",
      "Epoch [118/1], Loss: 0.30799317359924316\n",
      "Epoch [119/1], Loss: 0.5268716812133789\n",
      "Epoch [120/1], Loss: 0.46037206053733826\n",
      "Epoch [120/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [121/1], Loss: 0.4110344648361206\n",
      "Epoch [122/1], Loss: 0.363156259059906\n",
      "Epoch [123/1], Loss: 0.3923078775405884\n",
      "Epoch [124/1], Loss: 0.31867077946662903\n",
      "Epoch [125/1], Loss: 0.4923730194568634\n",
      "Epoch [126/1], Loss: 0.398714154958725\n",
      "Epoch [127/1], Loss: 0.27286025881767273\n",
      "Epoch [128/1], Loss: 0.41004133224487305\n",
      "Epoch [129/1], Loss: 0.33337435126304626\n",
      "Epoch [130/1], Loss: 0.2416802942752838\n",
      "Epoch [130/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [131/1], Loss: 0.4814128577709198\n",
      "Epoch [132/1], Loss: 0.3232722282409668\n",
      "Epoch [133/1], Loss: 0.3481864333152771\n",
      "Epoch [134/1], Loss: 0.4135742783546448\n",
      "Epoch [135/1], Loss: 0.3938474953174591\n",
      "Epoch [136/1], Loss: 0.2997336685657501\n",
      "Epoch [137/1], Loss: 0.42485782504081726\n",
      "Epoch [138/1], Loss: 0.5399637818336487\n",
      "Epoch [139/1], Loss: 0.2989731729030609\n",
      "Epoch [140/1], Loss: 0.3129267394542694\n",
      "Epoch [140/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [141/1], Loss: 0.3285888135433197\n",
      "Epoch [142/1], Loss: 0.8762134909629822\n",
      "Epoch [143/1], Loss: 0.3392188251018524\n",
      "Epoch [144/1], Loss: 0.3434174358844757\n",
      "Epoch [145/1], Loss: 0.4714681804180145\n",
      "Epoch [146/1], Loss: 0.5784721374511719\n",
      "Epoch [147/1], Loss: 0.3189379870891571\n",
      "Epoch [148/1], Loss: 0.3576185703277588\n",
      "Epoch [149/1], Loss: 0.488383412361145\n",
      "Epoch [150/1], Loss: 0.39265379309654236\n",
      "Epoch [150/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [151/1], Loss: 0.4309590458869934\n",
      "Epoch [152/1], Loss: 0.542587399482727\n",
      "Epoch [153/1], Loss: 0.3767068088054657\n",
      "Epoch [154/1], Loss: 0.5200443863868713\n",
      "Epoch [155/1], Loss: 0.478606641292572\n",
      "Epoch [156/1], Loss: 0.30187782645225525\n",
      "Epoch [157/1], Loss: 0.30378642678260803\n",
      "Epoch [158/1], Loss: 0.32547563314437866\n",
      "Epoch [159/1], Loss: 0.3177024722099304\n",
      "Epoch [160/1], Loss: 0.4648441672325134\n",
      "Epoch [160/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [161/1], Loss: 0.3479349911212921\n",
      "Epoch [162/1], Loss: 0.3898507058620453\n",
      "Epoch [163/1], Loss: 0.5959447622299194\n",
      "Epoch [164/1], Loss: 0.283836305141449\n",
      "Epoch [165/1], Loss: 0.37710654735565186\n",
      "Epoch [166/1], Loss: 0.4580022990703583\n",
      "Epoch [167/1], Loss: 0.3000998795032501\n",
      "Epoch [168/1], Loss: 0.37077170610427856\n",
      "Epoch [169/1], Loss: 0.31143906712532043\n",
      "Epoch [170/1], Loss: 0.3523285984992981\n",
      "Epoch [170/1], prec: 0.0011210762331838565, rec: 0.001181620969181417, f1: 0.001035875208771359\n",
      "Epoch [171/1], Loss: 0.2722106873989105\n",
      "Epoch [172/1], Loss: 0.32388272881507874\n",
      "Epoch [173/1], Loss: 0.48531782627105713\n",
      "Epoch [174/1], Loss: 0.33481067419052124\n",
      "Epoch [175/1], Loss: 0.27167075872421265\n",
      "Epoch [176/1], Loss: 0.4114868938922882\n",
      "Epoch [177/1], Loss: 0.2791862189769745\n",
      "Epoch [178/1], Loss: 0.4958023428916931\n",
      "Epoch [179/1], Loss: 0.5262811779975891\n",
      "Epoch [180/1], Loss: 0.27116596698760986\n",
      "Epoch [180/1], prec: 0.0008968609865470852, rec: 0.0009777889267843522, f1: 0.0008223368786411005\n",
      "Epoch [181/1], Loss: 0.45026883482933044\n",
      "Epoch [182/1], Loss: 0.6284446716308594\n",
      "Epoch [183/1], Loss: 0.4271378517150879\n",
      "Epoch [184/1], Loss: 0.41088947653770447\n",
      "Epoch [185/1], Loss: 0.6211904287338257\n",
      "Epoch [186/1], Loss: 0.3856782615184784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     restDat \u001b[38;5;241m=\u001b[39m rest_feature\n\u001b[1;32m     13\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(userDat, restDat)\n\u001b[0;32m---> 14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(prediction , label)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.03, weight_decay= 1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, batchDat in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data, label = batchDat\n",
    "        userDat = data.to(device)\n",
    "        label = label.to(device)\n",
    "        restDat = rest_feature\n",
    "        prediction = model(userDat, restDat)\n",
    "        loss = criterion(prediction , label)\n",
    "    loss.backward()\n",
    "    total_loss += loss.item() \n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}')\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        \n",
    "\t        lResults = evaluateModel(model, test_loader, rest_feature, groundtruth, test_users, config['quantity'], rest_Label)\n",
    "\n",
    "\t        p, r, f = extractResult(lResults)\n",
    "\t        if mean(r) > mean(mr):\n",
    "\t            mp, mr, mf = p, r, f\n",
    "\t        print(f'Epoch [{epoch+1}/{num_epochs}], prec: {mean(p)}, rec: {mean(r)}, f1: {mean(f)}')\n",
    "\t    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
